vocab:
  vocab_prefix: "PhoMT"
  unk_piece: "<UNK>"
  bos_piece: "<BOS>"
  eos_piece: "<EOS>"
  pad_piece: "<PAD>"
  path:
    train: "/kaggle/input/small-phomt/train_preprocessed.json"
    val:   "/kaggle/input/small-phomt/dev_preprocessed.json"
    test:  "/kaggle/input/small-phomt/test_preprocessed.json"
  source_text: "english"
  target_text: "vietnamese"
  task_type: "machine_translation" 

# --- Global Settings ---
seed: 42
checkpoint_dir: "/kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/"

# --- Dataset Definitions ---
datasets:
  pho_mt:
    train_path: "/kaggle/input/small-phomt/train_preprocessed.json"
    val_path: "/kaggle/input/small-phomt/dev_preprocessed.json"
    test_path: "/kaggle/input/small-phomt/test_preprocessed.json"

# --- Model Definitions ---
models:
  LSTM_Global_Attention: models.LSTM_Global_Attention.LSTM_Global_Attention 

# --- Experiment Definition ---
experiments:
  - name: "LSTM_Global_Attention_PhoMT"
    model: "LSTM_Global_Attention"
    dataset: "pho_mt"
    hyperparameters:
      epochs: 35
      early_stop_patience: 10
      lr: 0.001
      weight_decay: 0.00001
      batch_size: 32
      optimizer: "Adam"
      step_size: 30
      gamma: 0.1
      optimizer_params: {}
      embedding_dim: 256
      hidden_dim: 256
      encoder_layers: 3
      decoder_layers: 3
      dropout: 0.1
      max_length: 64
