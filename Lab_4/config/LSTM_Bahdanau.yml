vocab:
  vocab_prefix: "PhoMT"
  unk_piece: "<UNK>"
  bos_piece: "<BOS>"
  eos_piece: "<EOS>"
  pad_piece: "<PAD>"
  path:
    train: "/kaggle/input/small-phomt/train.json"
    val:   "/kaggle/input/small-phomt/dev.json"
    test:  "/kaggle/input/small-phomt/test.json"
  source_text: "english"
  target_text: "vietnamese"
  task_type: "machine_translation" 

# --- Global Settings ---
seed: 42
checkpoint_dir: "./checkpoints/LSTM_Bahdanau_PhoMT/"

# --- Dataset Definitions ---
datasets:
  pho_mt:
    train_path: "/kaggle/input/small-phomt/train.json"
    val_path: "/kaggle/input/small-phomt/dev.json"
    test_path: "/kaggle/input/small-phomt/test.json"

# --- Model Definitions ---
models:
  LSTM_Bahdanau: models.LSTM_Bahdanau.LSTM_Bahdanau 

# --- Experiment Definition ---
experiments:
  - name: "LSTM_Bahdanau_PhoMT"
    model: "LSTM_Bahdanau"
    dataset: "pho_mt"
    hyperparameters:
      epochs: 30
      early_stop_patience: 5
      lr: 0.001
      weight_decay: 0.00001
      batch_size: 32
      optimizer: "Adam"
      step_size: 30
      gamma: 0.1
      optimizer_params: {}
      embedding_dim: 256
      hidden_dim: 256
      encoder_layers: 3
      decoder_layers: 3
      dropout: 0.1
      max_length: 64
