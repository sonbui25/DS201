{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0fd1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:00:27.076885Z",
     "iopub.status.busy": "2025-12-18T11:00:27.076201Z",
     "iopub.status.idle": "2025-12-18T11:00:33.828981Z",
     "shell.execute_reply": "2025-12-18T11:00:33.828259Z"
    },
    "papermill": {
     "duration": 6.759097,
     "end_time": "2025-12-18T11:00:33.830818",
     "exception": false,
     "start_time": "2025-12-18T11:00:27.071721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.2)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\r\n",
      "Building wheels for collected packages: rouge_score\r\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=806b286463d99cdc764e8b6e639a98744cee4c79ae59bfb68b1e0bf1e81efb1b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\r\n",
      "Successfully built rouge_score\r\n",
      "Installing collected packages: rouge_score\r\n",
      "Successfully installed rouge_score-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c130fe30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-18T11:00:33.837089Z",
     "iopub.status.busy": "2025-12-18T11:00:33.836837Z",
     "iopub.status.idle": "2025-12-18T11:00:36.696954Z",
     "shell.execute_reply": "2025-12-18T11:00:36.696145Z"
    },
    "papermill": {
     "duration": 2.865458,
     "end_time": "2025-12-18T11:00:36.698904",
     "exception": false,
     "start_time": "2025-12-18T11:00:33.833446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DS201'...\r\n",
      "remote: Enumerating objects: 2071, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (113/113), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\r\n",
      "remote: Total 2071 (delta 69), reused 80 (delta 43), pack-reused 1958 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (2071/2071), 40.92 MiB | 36.79 MiB/s, done.\r\n",
      "Resolving deltas: 100% (1378/1378), done.\r\n"
     ]
    }
   ],
   "source": [
    "# Remove the directory if it exists\n",
    "!rm -rf DS201\n",
    "!rm -rf checkpoints\n",
    "# Clone the repository again\n",
    "!git clone https://github.com/sonbui25/DS201.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082b0533",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-18T11:00:36.706802Z",
     "iopub.status.busy": "2025-12-18T11:00:36.706192Z",
     "iopub.status.idle": "2025-12-18T11:36:17.089590Z",
     "shell.execute_reply": "2025-12-18T11:36:17.088579Z"
    },
    "papermill": {
     "duration": 2140.390167,
     "end_time": "2025-12-18T11:36:17.092196",
     "exception": false,
     "start_time": "2025-12-18T11:00:36.702029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\r\n",
      "STARTING PROCESS FOR EXPERIMENT: LSTM_PhoMT\r\n",
      "Log file location: /kaggle/working/DS201/Lab_4/results/LSTM_PhoMT/LSTM_PhoMT_training.log\r\n",
      "Loading configuration from: /kaggle/working/DS201/Lab_4/config/LSTM.yml\r\n",
      "Using device: cuda\r\n",
      "Random seed set to: 42\r\n",
      "\r\n",
      "Running Experiment: LSTM_PhoMT\r\n",
      "Model: LSTM, Dataset: pho_mt\r\n",
      "Hyperparameters: {'epochs': 35, 'early_stop_patience': 10, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 128, 'optimizer': 'Adam', 'step_size': 30, 'gamma': 0.1, 'optimizer_params': {}, 'embedding_dim': 256, 'hidden_dim': 256, 'encoder_layers': 3, 'decoder_layers': 3, 'dropout': 0.1, 'max_length': 64}\r\n",
      "Loading dataset: pho_mt\r\n",
      "Train: 20000, Val: 2000, Test: 2000\r\n",
      "Using optimizer: Adam with LR=0.001, WeightDecay=1e-05\r\n",
      "START TRAINING LSTM_PhoMT...\r\n",
      "No checkpoint found at /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_last_checkpoint.pth\r\n",
      "Epoch:   0%|                           | 0/35 [00:53<?, ?it/s, train_loss=1.934]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       0 |       1.9341 |          0.1179 |        0.0856 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 0 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:   3%|▌                  | 1/35 [01:47<31:35, 55.75s/it, train_loss=1.682]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       1 |       1.6818 |          0.1249 |        0.1091 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 1 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:   6%|█                  | 2/35 [02:41<30:08, 54.80s/it, train_loss=1.604]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       2 |       1.6037 |          0.1486 |        0.2052 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 2 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:   9%|█▋                 | 3/35 [03:37<29:08, 54.63s/it, train_loss=1.514]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       3 |       1.5136 |          0.2183 |        0.2591 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 3 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  11%|██▏                | 4/35 [04:35<28:34, 55.32s/it, train_loss=1.453]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       4 |        1.453 |          0.2745 |        0.3146 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 4 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  14%|██▋                | 5/35 [05:34<28:10, 56.34s/it, train_loss=1.389]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       5 |       1.3885 |          0.3128 |         0.314 |\r\n",
      "Epoch:  17%|███▎               | 6/35 [06:35<27:39, 57.22s/it, train_loss=1.319]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       6 |       1.3191 |          0.3264 |        0.3244 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 6 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  20%|███▊               | 7/35 [07:35<27:10, 58.24s/it, train_loss=1.287]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       7 |       1.2865 |          0.3391 |        0.3297 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 7 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  23%|████▎              | 8/35 [08:35<26:33, 59.03s/it, train_loss=1.285]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       8 |       1.2851 |          0.3438 |        0.3396 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 8 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  26%|████▉              | 9/35 [09:36<25:43, 59.35s/it, train_loss=1.210]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       9 |       1.2099 |          0.3609 |        0.3446 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 9 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  29%|█████▏            | 10/35 [10:38<24:56, 59.87s/it, train_loss=1.175]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      10 |       1.1748 |          0.3693 |         0.345 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 10 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  31%|█████▋            | 11/35 [11:39<24:07, 60.32s/it, train_loss=1.153]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      11 |       1.1532 |          0.3784 |        0.3395 |\r\n",
      "Epoch:  34%|██████▏           | 12/35 [12:40<23:11, 60.49s/it, train_loss=1.123]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      12 |       1.1231 |           0.387 |         0.341 |\r\n",
      "Epoch:  37%|██████▋           | 13/35 [13:41<22:15, 60.72s/it, train_loss=1.096]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      13 |       1.0959 |          0.3938 |        0.3386 |\r\n",
      "Epoch:  40%|███████▏          | 14/35 [14:42<21:18, 60.86s/it, train_loss=1.078]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      14 |       1.0778 |          0.4006 |        0.3472 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 14 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  43%|███████▋          | 15/35 [15:43<20:20, 61.02s/it, train_loss=1.075]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      15 |       1.0755 |          0.4079 |        0.3501 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 15 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  46%|████████▏         | 16/35 [16:45<19:19, 61.02s/it, train_loss=1.039]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      16 |       1.0395 |          0.4146 |        0.3484 |\r\n",
      "Epoch:  49%|████████▋         | 17/35 [17:47<18:21, 61.18s/it, train_loss=1.015]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      17 |       1.0152 |          0.4207 |        0.3537 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 17 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  51%|█████████▎        | 18/35 [18:48<17:23, 61.36s/it, train_loss=1.009]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      18 |       1.0087 |          0.4265 |        0.3561 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 18 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  54%|█████████▊        | 19/35 [19:49<16:22, 61.39s/it, train_loss=0.992]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      19 |       0.9924 |          0.4336 |        0.3583 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 19 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  57%|██████████▎       | 20/35 [20:51<15:20, 61.37s/it, train_loss=0.977]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      20 |       0.9771 |          0.4384 |         0.359 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 20 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  60%|██████████▊       | 21/35 [21:52<14:19, 61.36s/it, train_loss=0.952]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      21 |       0.9523 |          0.4445 |        0.3597 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 21 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  63%|███████████▎      | 22/35 [22:55<13:19, 61.48s/it, train_loss=0.926]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      22 |       0.9255 |          0.4497 |        0.3662 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 22 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  66%|███████████▊      | 23/35 [23:57<12:20, 61.74s/it, train_loss=0.934]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      23 |       0.9344 |          0.4537 |        0.3644 |\r\n",
      "Epoch:  69%|████████████▎     | 24/35 [24:58<11:18, 61.72s/it, train_loss=0.921]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      24 |       0.9212 |          0.4593 |        0.3671 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 24 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  71%|████████████▊     | 25/35 [26:00<10:16, 61.67s/it, train_loss=0.896]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      25 |       0.8957 |          0.4651 |         0.367 |\r\n",
      "Epoch:  74%|█████████████▎    | 26/35 [27:02<09:15, 61.72s/it, train_loss=0.878]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      26 |       0.8783 |          0.4703 |        0.3646 |\r\n",
      "Epoch:  77%|█████████████▉    | 27/35 [28:04<08:14, 61.78s/it, train_loss=0.869]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      27 |       0.8692 |          0.4767 |        0.3697 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 27 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  80%|██████████████▍   | 28/35 [29:06<07:12, 61.82s/it, train_loss=0.845]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      28 |       0.8449 |            0.48 |        0.3692 |\r\n",
      "Epoch:  83%|██████████████▉   | 29/35 [30:08<06:12, 62.03s/it, train_loss=0.859]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      29 |       0.8586 |          0.4825 |        0.3703 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 29 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  86%|███████████████▍  | 30/35 [31:10<05:09, 61.95s/it, train_loss=0.800]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      30 |       0.7999 |          0.5053 |        0.3722 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 30 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  89%|███████████████▉  | 31/35 [32:12<04:07, 61.98s/it, train_loss=0.787]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      31 |       0.7869 |          0.5105 |        0.3732 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 31 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  91%|████████████████▍ | 32/35 [33:14<03:05, 61.99s/it, train_loss=0.777]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      32 |       0.7765 |          0.5141 |        0.3733 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 32 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch:  94%|████████████████▉ | 33/35 [34:17<02:04, 62.09s/it, train_loss=0.775]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      33 |       0.7754 |          0.5166 |         0.373 |\r\n",
      "Epoch:  97%|█████████████████▍| 34/35 [35:19<01:02, 62.17s/it, train_loss=0.777]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      34 |       0.7768 |          0.5189 |        0.3739 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 34 to: /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth\r\n",
      "Epoch: 100%|██████████████████| 35/35 [35:22<00:00, 60.64s/it, train_loss=0.777]\r\n",
      "\r\n",
      "Best result based on ROUGE-L on validation set:\r\n",
      "Best epoch: 34, Val_ROUGE-L: 0.3739\r\n",
      "DONE TRAINING LSTM_PhoMT. Ran for 35 epochs.\r\n",
      "\r\n",
      " Starting final evaluation on the (unseen) test set...\r\n",
      "Loading best model from /kaggle/working/DS201/checkpoints/LSTM_PhoMT/LSTM_PhoMT_best_model.pth for final test evaluation...\r\n",
      " Best model loaded successfully.\r\n",
      "Evaluating Test Set: 100%|██████████████████████| 16/16 [00:02<00:00,  5.82it/s]\r\n",
      "Predictions log saved to /kaggle/working/DS201/Lab_4/results/LSTM_PhoMT/LSTM_PhoMT_test_predictions.jsonl\r\n",
      " Test evaluation completed successfully.\r\n",
      " Test Set Results: ROUGE-L: 0.3854\r\n",
      "Plotting training/validation results for LSTM_PhoMT.\r\n",
      "Metrics figure saved at: /kaggle/working/DS201/Lab_4/results/LSTM_PhoMT_pho_mt_metrics.png\r\n",
      "Figure(1800x1000)\r\n",
      "Figure(1200x500)\r\n",
      "\r\n",
      "Experiment LSTM_PhoMT finished successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/DS201/Lab_4/main.py --config '/kaggle/working/DS201/Lab_4/config/LSTM.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f70d3ea",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-18T11:36:17.586409Z",
     "iopub.status.busy": "2025-12-18T11:36:17.585710Z",
     "iopub.status.idle": "2025-12-18T11:36:38.246108Z",
     "shell.execute_reply": "2025-12-18T11:36:38.245103Z"
    },
    "papermill": {
     "duration": 20.868153,
     "end_time": "2025-12-18T11:36:38.248024",
     "exception": false,
     "start_time": "2025-12-18T11:36:17.379871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\r\n",
      "STARTING PROCESS FOR EXPERIMENT: LSTM_Bahdanau_PhoMT\r\n",
      "Log file location: /kaggle/working/DS201/Lab_4/results/LSTM_Bahdanau_PhoMT/LSTM_Bahdanau_PhoMT_training.log\r\n",
      "Loading configuration from: /kaggle/working/DS201/Lab_4/config/LSTM_Bahdanau.yml\r\n",
      "Using device: cuda\r\n",
      "Random seed set to: 42\r\n",
      "\r\n",
      "Running Experiment: LSTM_Bahdanau_PhoMT\r\n",
      "Model: LSTM_Bahdanau, Dataset: pho_mt\r\n",
      "Hyperparameters: {'epochs': 35, 'early_stop_patience': 10, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 128, 'optimizer': 'Adam', 'step_size': 30, 'gamma': 0.1, 'optimizer_params': {}, 'embedding_dim': 256, 'hidden_dim': 256, 'encoder_layers': 3, 'decoder_layers': 3, 'dropout': 0.1, 'max_length': 64}\r\n",
      "Loading dataset: pho_mt\r\n",
      "Train: 20000, Val: 2000, Test: 2000\r\n",
      "Using optimizer: Adam with LR=0.001, WeightDecay=1e-05\r\n",
      "START TRAINING LSTM_Bahdanau_PhoMT...\r\n",
      "No checkpoint found at /kaggle/working/DS201/checkpoints/LSTM_Bahdanau_PhoMT/LSTM_Bahdanau_PhoMT_last_checkpoint.pth\r\n",
      "Epoch:   0%|                           | 0/35 [00:13<?, ?it/s, train_loss=4.835]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/main.py\", line 290, in <module>\r\n",
      "    main()\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/main.py\", line 260, in main\r\n",
      "    results, actual_epochs_ran = trainer.train(\r\n",
      "                                 ^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/tasks/seq2seq_engine.py\", line 169, in train\r\n",
      "    train_loss, train_rouge_L = self.train_step(epoch_pbar=pbar)\r\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/tasks/seq2seq_engine.py\", line 68, in train_step\r\n",
      "    loss.backward()\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 647, in backward\r\n",
      "    torch.autograd.backward(\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\r\n",
      "    _engine_run_backward(\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\r\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 17261 has 15.72 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/DS201/Lab_4/main.py --config '/kaggle/working/DS201/Lab_4/config/LSTM_Bahdanau.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1a23c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:36:38.646380Z",
     "iopub.status.busy": "2025-12-18T11:36:38.646037Z",
     "iopub.status.idle": "2025-12-18T11:37:33.244031Z",
     "shell.execute_reply": "2025-12-18T11:37:33.243288Z"
    },
    "papermill": {
     "duration": 54.797567,
     "end_time": "2025-12-18T11:37:33.245770",
     "exception": false,
     "start_time": "2025-12-18T11:36:38.448203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\r\n",
      "STARTING PROCESS FOR EXPERIMENT: LSTM_Local_Attention_PhoMT\r\n",
      "Log file location: /kaggle/working/DS201/Lab_4/results/LSTM_Local_Attention_PhoMT/LSTM_Local_Attention_PhoMT_training.log\r\n",
      "Loading configuration from: /kaggle/working/DS201/Lab_4/config/LSTM_Local_Attention.yml\r\n",
      "Using device: cuda\r\n",
      "Random seed set to: 42\r\n",
      "\r\n",
      "Running Experiment: LSTM_Local_Attention_PhoMT\r\n",
      "Model: LSTM_Local_Attention, Dataset: pho_mt\r\n",
      "Hyperparameters: {'epochs': 35, 'early_stop_patience': 10, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 128, 'optimizer': 'Adam', 'step_size': 30, 'gamma': 0.1, 'optimizer_params': {}, 'embedding_dim': 256, 'hidden_dim': 256, 'encoder_layers': 3, 'decoder_layers': 3, 'dropout': 0.1, 'D': 10, 'max_length': 64}\r\n",
      "Loading dataset: pho_mt\r\n",
      "Train: 20000, Val: 2000, Test: 2000\r\n",
      "Using optimizer: Adam with LR=0.001, WeightDecay=1e-05\r\n",
      "START TRAINING LSTM_Local_Attention_PhoMT...\r\n",
      "No checkpoint found at /kaggle/working/DS201/checkpoints/LSTM_Local_Attention_PhoMT/LSTM_Local_Attention_PhoMT_last_checkpoint.pth\r\n",
      "Epoch:   0%|                           | 0/35 [00:47<?, ?it/s, train_loss=3.149]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/main.py\", line 290, in <module>\r\n",
      "    main()\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/main.py\", line 260, in main\r\n",
      "    results, actual_epochs_ran = trainer.train(\r\n",
      "                                 ^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/tasks/seq2seq_engine.py\", line 169, in train\r\n",
      "    train_loss, train_rouge_L = self.train_step(epoch_pbar=pbar)\r\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/tasks/seq2seq_engine.py\", line 55, in train_step\r\n",
      "    y_pred = self.model(X, y) # (batch_size, seq_len, vocab_size)\r\n",
      "             ^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\r\n",
      "    return self._call_impl(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/DS201/Lab_4/models/LSTM_Local_Attention.py\", line 205, in forward\r\n",
      "    enc_outputs_unpacked, _ = pad_packed_sequence(enc_outputs, batch_first=True, total_length=src_len)\r\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/utils/rnn.py\", line 407, in pad_packed_sequence\r\n",
      "    padded_output.index_select(batch_dim, unsorted_indices),\r\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 31.12 MiB is free. Process 17428 has 15.86 GiB memory in use. Of the allocated memory 15.30 GiB is allocated by PyTorch, and 278.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/DS201/Lab_4/main.py --config '/kaggle/working/DS201/Lab_4/config/LSTM_Local_Attention.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db22a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:37:33.653546Z",
     "iopub.status.busy": "2025-12-18T11:37:33.652772Z",
     "iopub.status.idle": "2025-12-18T12:26:47.539289Z",
     "shell.execute_reply": "2025-12-18T12:26:47.538347Z"
    },
    "papermill": {
     "duration": 2954.090727,
     "end_time": "2025-12-18T12:26:47.541258",
     "exception": false,
     "start_time": "2025-12-18T11:37:33.450531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\r\n",
      "STARTING PROCESS FOR EXPERIMENT: LSTM_Global_Attention_PhoMT\r\n",
      "Log file location: /kaggle/working/DS201/Lab_4/results/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_training.log\r\n",
      "Loading configuration from: /kaggle/working/DS201/Lab_4/config/LSTM_Global_Attention.yml\r\n",
      "Using device: cuda\r\n",
      "Random seed set to: 42\r\n",
      "\r\n",
      "Running Experiment: LSTM_Global_Attention_PhoMT\r\n",
      "Model: LSTM_Global_Attention, Dataset: pho_mt\r\n",
      "Hyperparameters: {'epochs': 35, 'early_stop_patience': 10, 'lr': 0.001, 'weight_decay': 1e-05, 'batch_size': 128, 'optimizer': 'Adam', 'step_size': 30, 'gamma': 0.1, 'optimizer_params': {}, 'embedding_dim': 256, 'hidden_dim': 256, 'encoder_layers': 3, 'decoder_layers': 3, 'dropout': 0.1, 'max_length': 64}\r\n",
      "Loading dataset: pho_mt\r\n",
      "Train: 20000, Val: 2000, Test: 2000\r\n",
      "Using optimizer: Adam with LR=0.001, WeightDecay=1e-05\r\n",
      "START TRAINING LSTM_Global_Attention_PhoMT...\r\n",
      "No checkpoint found at /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_last_checkpoint.pth\r\n",
      "Epoch:   0%|                           | 0/35 [01:15<?, ?it/s, train_loss=1.964]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       0 |       1.9644 |          0.0749 |        0.1185 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 0 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:   3%|▌                  | 1/35 [02:33<44:24, 78.36s/it, train_loss=1.590]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       1 |       1.5904 |          0.1894 |        0.2688 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 1 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:   6%|█                  | 2/35 [03:53<43:18, 78.75s/it, train_loss=1.530]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       2 |       1.5299 |          0.2518 |        0.2575 |\r\n",
      "Epoch:   9%|█▋                 | 3/35 [05:15<42:13, 79.18s/it, train_loss=1.408]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       3 |       1.4076 |          0.2928 |        0.2804 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 3 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  11%|██▏                | 4/35 [06:37<41:25, 80.19s/it, train_loss=1.346]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       4 |       1.3456 |          0.3244 |        0.3225 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 4 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  14%|██▋                | 5/35 [08:00<40:32, 81.08s/it, train_loss=1.288]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       5 |       1.2881 |          0.3443 |        0.3139 |\r\n",
      "Epoch:  17%|███▎               | 6/35 [09:23<39:24, 81.55s/it, train_loss=1.212]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       6 |       1.2124 |          0.3624 |        0.3284 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 6 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  20%|███▊               | 7/35 [10:48<38:21, 82.19s/it, train_loss=1.152]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       7 |       1.1516 |          0.3771 |         0.329 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 7 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  23%|████▎              | 8/35 [12:12<37:21, 83.01s/it, train_loss=1.122]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       8 |       1.1215 |          0.3894 |        0.3346 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 8 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  26%|████▉              | 9/35 [13:36<36:07, 83.38s/it, train_loss=1.089]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|       9 |       1.0893 |          0.4004 |        0.3403 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 9 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  29%|█████▏            | 10/35 [15:01<34:51, 83.65s/it, train_loss=1.038]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      10 |       1.0385 |          0.4111 |        0.3348 |\r\n",
      "Epoch:  31%|█████▋            | 11/35 [16:25<33:32, 83.87s/it, train_loss=1.028]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      11 |       1.0283 |          0.4202 |        0.3402 |\r\n",
      "Epoch:  34%|██████▏           | 12/35 [17:48<32:06, 83.77s/it, train_loss=1.008]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      12 |       1.0077 |           0.428 |        0.3437 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 12 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  37%|██████▋           | 13/35 [19:12<30:42, 83.74s/it, train_loss=0.963]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      13 |       0.9628 |          0.4369 |        0.3475 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 13 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  40%|███████▏          | 14/35 [20:36<29:21, 83.88s/it, train_loss=0.941]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      14 |       0.9411 |          0.4448 |        0.3517 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 14 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  43%|███████▋          | 15/35 [22:02<27:59, 83.97s/it, train_loss=1.125]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      15 |       1.1252 |            0.42 |        0.3423 |\r\n",
      "Epoch:  46%|████████▏         | 16/35 [23:26<26:40, 84.22s/it, train_loss=0.960]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      16 |         0.96 |          0.4343 |        0.3578 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 16 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  49%|████████▋         | 17/35 [24:50<25:18, 84.34s/it, train_loss=0.896]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      17 |       0.8964 |          0.4636 |        0.3654 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 17 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  51%|█████████▎        | 18/35 [26:15<23:52, 84.27s/it, train_loss=0.837]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      18 |       0.8367 |          0.4829 |        0.3799 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 18 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  54%|█████████▊        | 19/35 [27:41<22:32, 84.55s/it, train_loss=0.784]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      19 |       0.7838 |          0.5015 |        0.3922 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 19 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  57%|██████████▎       | 20/35 [29:05<21:12, 84.84s/it, train_loss=0.765]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      20 |       0.7646 |          0.5178 |         0.406 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 20 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  60%|██████████▊       | 21/35 [30:30<19:45, 84.68s/it, train_loss=0.720]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      21 |       0.7202 |          0.5321 |        0.4183 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 21 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  63%|███████████▎      | 22/35 [31:56<18:23, 84.88s/it, train_loss=0.690]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      22 |       0.6897 |          0.5475 |         0.422 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 22 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  66%|███████████▊      | 23/35 [33:21<16:59, 84.99s/it, train_loss=0.667]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      23 |       0.6666 |          0.5569 |        0.4285 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 23 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  69%|████████████▎     | 24/35 [34:47<15:35, 85.03s/it, train_loss=0.632]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      24 |       0.6317 |          0.5676 |        0.4413 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 24 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  71%|████████████▊     | 25/35 [36:12<14:14, 85.40s/it, train_loss=0.618]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      25 |       0.6179 |          0.5801 |        0.4417 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 25 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  74%|█████████████▎    | 26/35 [37:37<12:46, 85.17s/it, train_loss=0.592]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      26 |       0.5916 |          0.5915 |        0.4463 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 26 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  77%|█████████████▉    | 27/35 [39:01<11:20, 85.11s/it, train_loss=0.578]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      27 |       0.5776 |           0.602 |        0.4494 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 27 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  80%|██████████████▍   | 28/35 [40:27<09:54, 84.95s/it, train_loss=0.557]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      28 |       0.5566 |          0.6089 |        0.4519 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 28 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  83%|██████████████▉   | 29/35 [41:52<08:30, 85.07s/it, train_loss=0.531]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      29 |       0.5305 |          0.6186 |         0.458 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 29 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  86%|███████████████▍  | 30/35 [43:18<07:06, 85.36s/it, train_loss=0.465]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      30 |       0.4653 |          0.6592 |         0.465 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 30 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  89%|███████████████▉  | 31/35 [44:43<05:41, 85.34s/it, train_loss=0.440]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      31 |       0.4403 |          0.6723 |        0.4659 |\r\n",
      "\r\n",
      "[INFO]: Saving best model at epoch 31 to: /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth\r\n",
      "Epoch:  91%|████████████████▍ | 32/35 [46:09<04:16, 85.37s/it, train_loss=0.429]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      32 |        0.429 |          0.6779 |        0.4653 |\r\n",
      "Epoch:  94%|████████████████▉ | 33/35 [47:34<02:50, 85.47s/it, train_loss=0.426]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      33 |       0.4262 |          0.6835 |        0.4643 |\r\n",
      "Epoch:  97%|█████████████████▍| 34/35 [48:59<01:25, 85.23s/it, train_loss=0.417]\r\n",
      "|   Epoch |   Train Loss |   Train ROUGE-L |   Val ROUGE-L |\r\n",
      "|---------|--------------|-----------------|---------------|\r\n",
      "|      34 |       0.4169 |          0.6872 |        0.4652 |\r\n",
      "Epoch: 100%|██████████████████| 35/35 [49:03<00:00, 84.10s/it, train_loss=0.417]\r\n",
      "\r\n",
      "Best result based on ROUGE-L on validation set:\r\n",
      "Best epoch: 31, Val_ROUGE-L: 0.4659\r\n",
      "DONE TRAINING LSTM_Global_Attention_PhoMT. Ran for 35 epochs.\r\n",
      "\r\n",
      " Starting final evaluation on the (unseen) test set...\r\n",
      "Loading best model from /kaggle/working/DS201/checkpoints/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_best_model.pth for final test evaluation...\r\n",
      " Best model loaded successfully.\r\n",
      "Evaluating Test Set: 100%|██████████████████████| 16/16 [00:03<00:00,  5.02it/s]\r\n",
      "Predictions log saved to /kaggle/working/DS201/Lab_4/results/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_test_predictions.jsonl\r\n",
      " Test evaluation completed successfully.\r\n",
      " Test Set Results: ROUGE-L: 0.4806\r\n",
      "Plotting training/validation results for LSTM_Global_Attention_PhoMT.\r\n",
      "Metrics figure saved at: /kaggle/working/DS201/Lab_4/results/LSTM_Global_Attention_PhoMT_pho_mt_metrics.png\r\n",
      "Figure(1800x1000)\r\n",
      "Figure(1200x500)\r\n",
      "\r\n",
      "Experiment LSTM_Global_Attention_PhoMT finished successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/DS201/Lab_4/main.py --config '/kaggle/working/DS201/Lab_4/config/LSTM_Global_Attention.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae459f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:48.436493Z",
     "iopub.status.busy": "2025-12-18T12:26:48.436164Z",
     "iopub.status.idle": "2025-12-18T12:26:49.017771Z",
     "shell.execute_reply": "2025-12-18T12:26:49.016953Z"
    },
    "papermill": {
     "duration": 0.993849,
     "end_time": "2025-12-18T12:26:49.019352",
     "exception": false,
     "start_time": "2025-12-18T12:26:48.025503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/DS201/Lab_4\n",
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31mresults/\u001b[m\r\n",
      "\t\u001b[31m../checkpoints/\u001b[m\r\n",
      "\r\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\r\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/DS201/Lab_4\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772c5a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:49.850986Z",
     "iopub.status.busy": "2025-12-18T12:26:49.850152Z",
     "iopub.status.idle": "2025-12-18T12:26:50.230610Z",
     "shell.execute_reply": "2025-12-18T12:26:50.229925Z"
    },
    "papermill": {
     "duration": 0.806389,
     "end_time": "2025-12-18T12:26:50.232124",
     "exception": false,
     "start_time": "2025-12-18T12:26:49.425735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.pth*\r\n",
      "checkpoint/checkpoints/*.pth\r\n"
     ]
    }
   ],
   "source": [
    "# Create .gitignore if not exit \n",
    "!touch .gitignore\n",
    "\n",
    "# Add \"checkpoints/*.pth\" if not exit\n",
    "!grep -qxF \"checkpoints/*.pth\" .gitignore || echo \"checkpoints/*.pth\" >> .gitignore\n",
    "\n",
    "# View content\n",
    "!cat .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d010c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:51.126005Z",
     "iopub.status.busy": "2025-12-18T12:26:51.125690Z",
     "iopub.status.idle": "2025-12-18T12:26:51.243371Z",
     "shell.execute_reply": "2025-12-18T12:26:51.242703Z"
    },
    "papermill": {
     "duration": 0.528866,
     "end_time": "2025-12-18T12:26:51.244927",
     "exception": false,
     "start_time": "2025-12-18T12:26:50.716061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   config\t data_preprocessing.py\t__init__.py  models   tasks\r\n",
      "..  dataloaders  .gitignore\t\tmain.py      results  utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3fa9a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:52.053727Z",
     "iopub.status.busy": "2025-12-18T12:26:52.053123Z",
     "iopub.status.idle": "2025-12-18T12:26:52.437180Z",
     "shell.execute_reply": "2025-12-18T12:26:52.436203Z"
    },
    "papermill": {
     "duration": 0.795195,
     "end_time": "2025-12-18T12:26:52.439244",
     "exception": false,
     "start_time": "2025-12-18T12:26:51.644049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote\n",
    "\n",
    "load_dotenv(\"/kaggle/input/env-variables/.env\")\n",
    "\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "user_name = os.getenv(\"USER_NAME\")\n",
    "user_email = os.getenv(\"USER_EMAIL\")\n",
    "#print(\"Loaded GITHUB_TOKEN:\", os.getenv(\"GITHUB_TOKEN\")[:6] + \"*****\") # Show first n token\n",
    "\n",
    "!git config --global user.name \"{user_name}\"\n",
    "!git config --global user.email \"{user_email}\"\n",
    "\n",
    "repo_url = f\"https://{token}@github.com/sonbui25/DS201.git\"\n",
    "!git remote set-url origin {repo_url}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4da796b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:53.317949Z",
     "iopub.status.busy": "2025-12-18T12:26:53.317595Z",
     "iopub.status.idle": "2025-12-18T12:26:54.807072Z",
     "shell.execute_reply": "2025-12-18T12:26:54.806411Z"
    },
    "papermill": {
     "duration": 1.898028,
     "end_time": "2025-12-18T12:26:54.808598",
     "exception": false,
     "start_time": "2025-12-18T12:26:52.910570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 5ed32ac] Update Lab_4 results\r\n",
      " 9 files changed, 20477 insertions(+), 1 deletion(-)\r\n",
      " create mode 100644 Lab_4/results/LSTM_Bahdanau_PhoMT/LSTM_Bahdanau_PhoMT_training.log\r\n",
      " create mode 100644 Lab_4/results/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_test_predictions.jsonl\r\n",
      " create mode 100644 Lab_4/results/LSTM_Global_Attention_PhoMT/LSTM_Global_Attention_PhoMT_training.log\r\n",
      " create mode 100644 Lab_4/results/LSTM_Global_Attention_PhoMT_pho_mt_metrics.png\r\n",
      " create mode 100644 Lab_4/results/LSTM_Local_Attention_PhoMT/LSTM_Local_Attention_PhoMT_training.log\r\n",
      " create mode 100644 Lab_4/results/LSTM_PhoMT/LSTM_PhoMT_test_predictions.jsonl\r\n",
      " create mode 100644 Lab_4/results/LSTM_PhoMT/LSTM_PhoMT_training.log\r\n",
      " create mode 100644 Lab_4/results/LSTM_PhoMT_pho_mt_metrics.png\r\n",
      "Enumerating objects: 20, done.\r\n",
      "Counting objects: 100% (20/20), done.\r\n",
      "Delta compression using up to 4 threads\r\n",
      "Compressing objects: 100% (16/16), done.\r\n",
      "Writing objects: 100% (17/17), 464.29 KiB | 4.84 MiB/s, done.\r\n",
      "Total 17 (delta 6), reused 0 (delta 0), pack-reused 0\r\n",
      "remote: Resolving deltas: 100% (6/6), completed with 2 local objects.\u001b[K\r\n",
      "To https://github.com/sonbui25/DS201.git\r\n",
      "   bf6a474..5ed32ac  main -> main\r\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Update Lab_4 results\"\n",
    "!git push origin main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8bee2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:55.706968Z",
     "iopub.status.busy": "2025-12-18T12:26:55.706646Z",
     "iopub.status.idle": "2025-12-18T12:26:55.822901Z",
     "shell.execute_reply": "2025-12-18T12:26:55.822177Z"
    },
    "papermill": {
     "duration": 0.619317,
     "end_time": "2025-12-18T12:26:55.824225",
     "exception": false,
     "start_time": "2025-12-18T12:26:55.204908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/kaggle/working/DS201/Lab_4/checkpoints'\n",
      "/kaggle/working/DS201/Lab_4\n",
      "config\t     data_preprocessing.py  main.py  results  utils\r\n",
      "dataloaders  __init__.py\t    models   tasks\r\n"
     ]
    }
   ],
   "source": [
    "#Check model files (includes last checkpoint and best model file of each experiment)\n",
    "%cd /kaggle/working/DS201/Lab_4/checkpoints\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df35da85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:26:56.631891Z",
     "iopub.status.busy": "2025-12-18T12:26:56.631121Z",
     "iopub.status.idle": "2025-12-18T12:26:56.637641Z",
     "shell.execute_reply": "2025-12-18T12:26:56.636957Z"
    },
    "papermill": {
     "duration": 0.414979,
     "end_time": "2025-12-18T12:26:56.639057",
     "exception": false,
     "start_time": "2025-12-18T12:26:56.224078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!cd /kaggle/working/DS201\\n!git rm --cached Lab_4/checkpoints/*.pth\\n!git commit -m \"Ignore checkpoint files (>100MB)\"\\n!git remote set-url origin {repo_url}\\n!git push -f\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove model files .pth (remember to commit all before use)\n",
    "'''\n",
    "!cd /kaggle/working/DS201\n",
    "!git filter-branch --force --index-filter \\\n",
    "\"git rm --cached --ignore-unmatch Lab_4/checkpoints/*.pth\" \\\n",
    "--prune-empty --tag-name-filter cat -- --all\n",
    "''' \n",
    "# Command to remove model files\n",
    "'''\n",
    "!cd /kaggle/working/DS201\n",
    "!git rm --cached Lab_4/checkpoints/*.pth\n",
    "!git commit -m \"Ignore checkpoint files (>100MB)\"\n",
    "!git remote set-url origin {repo_url}\n",
    "!git push -f\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8603395,
     "sourceId": 13546706,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9054735,
     "sourceId": 14198464,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5192.715569,
   "end_time": "2025-12-18T12:26:57.258304",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-18T11:00:24.542735",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
